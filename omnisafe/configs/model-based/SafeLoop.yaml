# Copyright 2022 OmniSafe Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

defaults:
  # Basic Configurations
  ## Basic configurations for base class PG
  max_real_time_step: 1000000
  start_timesteps: 50000
  action_repeat: 5
  seed: 0
  device: "cpu" ##only support cpu now, gpu not checked yet
  sac_lr: 0.001
  alpha_init: 0.2
  gamma: 0.99
  polyak: 0.995
  use_bc_loss: False
  dynamics_freq: 1250
  policy_update_freq: 250
  policy_update_time: 50
  sac_config_evaluation_mode: None
  batch_size: 256
  eval_freq: 15000

  data_dir: "./runs"

  use_cost_critic: False
  reward_penalty: False
  scale_rewards: False
  standardized_obs: False

  ac_hidden_sizes: [64, 64]
  replay_size: 1000000

  ## Configuration For dynamics model
  dynamics_cfgs:
    reward_size: 1
    cost_size: 0
    state_dim: 26
    num_networks: 7
    num_elites: 5
    pred_hidden_size: 200
    use_decay: True

  # MPC controller configuration
  mpc_config:
    horizon: 8 # how long of the horizon to predict
    reward_horizon: 8
    gamma: 0.99 # reward discount coefficient
    exploration_noise: 0.0
    ARC:
      popsize: 100 # how many random samples for mpc
      particles: 4 # number of particles to enlarge
      max_iters: 8
      alpha: 0.1 # weights for previous mean and var
      mixture_coefficient: 0.05
      kappa: 1
