# Copyright 2022 OmniSafe Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

defaults:
  # Basic Configurations
  epochs: 500
  steps_per_epoch: 30000
  actor_iters: 80
  critic_iters: 40
  check_freq: 25
  save_freq: 100
  entropy_coef: 0.01
  max_ep_len: 1000
  num_mini_batches: 32
  actor_lr: 0.0003
  critic_lr: 0.001
  target_kl: 0.01
  data_dir: "./runs"
  seed: 0

  # Optional Configuration
  ## Whether to use cost critic
  use_cost: False
  cost_gamma: 1.0
  use_cost_critic: False
  linear_lr_decay: False
  exploration_noise_anneal: False
  reward_penalty: False
  kl_early_stopping: False
  use_max_grad_norm: False
  max_grad_norm: 0.5
  scale_rewards: False
  standardized_obs: True
  ## Configuration For Mode
  model_cfgs:
    shared_weights: False
    weight_initialization_mode: "kaiming_uniform"
    ac_kwargs:
      pi:
        actor_type: gaussian_learning
        hidden_sizes: [64, 64]
        activation: tanh
      val:
        hidden_sizes: [64, 64]
        activation: tanh
  ## Configuration For Buffer
  buffer_cfgs:
    gamma: 0.99
    lam: 0.95
    lam_c: 0.95
    adv_estimation_method: gae
    standardized_reward: False
    standardized_cost: False
    reward_penalty: False
